{
    "sourceFile": "src/S3ops.cpp",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1628035160029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1628035168116,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -332,9 +332,9 @@\n     } else {\n         std::stringstream ss;\n         auto err = requestPtr->GetLastError();\n         ss << \"[S3] Failed to get file: \" << path << \" because of: \" << err.GetMessage() <<'.';\n-        LogError(_context, ss.str());\n+        LogError(ss.str());\n     }\n \n     std::remove(tempstr.c_str());\n \n"
                },
                {
                    "date": 1628035182696,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -75,9 +75,9 @@\n     aws_client_config.requestTimeoutMs = 600000;\n     aws_client_config.caPath = Aws::String(\"/etc/ssl/certs/\");\n \n     if (!s3_access_key.empty() && !s3_secret_key.empty()) {\n-        LogInfo(_context, \"[S3] Using credentials from the config file\");\n+        LogInfo(\"[S3] Using credentials from the config file\");\n         s3_client = Aws::MakeShared<Aws::S3::S3Client>(\n                     ALLOCATION_TAG,\n                     Aws::Auth::AWSCredentials(s3_access_key.c_str(), s3_secret_key.c_str()),\n                     aws_client_config,\n@@ -85,9 +85,9 @@\n                     false //useVirtualAdressing\n                     );\n \n     } else {\n-        LogInfo(_context, \"No credentials in the config file. Falling back to ~/.aws/credentials or env variables.\");\n+        LogInfo(\"No credentials in the config file. Falling back to ~/.aws/credentials or env variables.\");\n         s3_client = Aws::MakeShared<Aws::S3::S3Client>(ALLOCATION_TAG,\n                                                        aws_client_config,\n                                                        Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n                                                        false //useVirtualAdressing\n@@ -97,9 +97,9 @@\n     _bucket_name = s3_bucket_name.c_str();\n \n     std::stringstream ss;\n     ss <<  \"[S3] Checking bucket: \" << _bucket_name;\n-    LogInfo(_context, ss.str().c_str());\n+    LogInfo(ss.str().c_str());\n \n     //Create bucket if it doesn't exist\n     //and verify if it exists\n     Aws::S3::Model::CreateBucketRequest request;\n@@ -118,19 +118,19 @@\n     if (outcome.GetError().GetErrorType() == Aws::S3::S3Errors::BUCKET_ALREADY_OWNED_BY_YOU ||\n             outcome.GetError().GetErrorType() == Aws::S3::S3Errors::BUCKET_ALREADY_EXISTS ) {\n         std::stringstream ss;\n         ss << \"[S3] Bucket exists: \" << _bucket_name;\n-        LogInfo(_context, ss.str().c_str());\n+        LogInfo(ss.str().c_str());\n     } else if (outcome.IsSuccess()) {\n         std::stringstream ss;\n         ss << \"[S3] Bucket created: \" << _bucket_name;\n-        LogInfo(_context, ss.str().c_str());\n+        LogInfo(ss.str().c_str());\n     } else {\n         std::stringstream err;\n         err << \"[S3] Create Bucket error: \" <<\n                outcome.GetError().GetExceptionName() << \" \" <<\n                outcome.GetError().GetMessage();\n-        LogError(_context, err.str().c_str());\n+        LogError(err.str().c_str());\n         return false;\n     }\n \n     return true;\n@@ -160,9 +160,9 @@\n         std::stringstream err;\n         err << \"[S3] PUT error: \" <<\n                put_object_outcome.GetError().GetExceptionName() << \" \" <<\n                put_object_outcome.GetError().GetMessage();\n-        LogError(_context, err.str().c_str());\n+        LogError(err.str().c_str());\n \n         return false;\n     }\n \n@@ -197,9 +197,9 @@\n         std::stringstream err;\n         err << \"[S3] GET error: \" <<\n                get_object_outcome.GetError().GetExceptionName() << \" \" <<\n                get_object_outcome.GetError().GetMessage();\n-        LogError(_context, err.str().c_str());\n+        LogError(err.str().c_str());\n \n         return false;\n     }\n \n@@ -218,9 +218,9 @@\n         std::stringstream err;\n         err << \"[S3] DELETE error: \" <<\n                delete_object_outcome.GetError().GetExceptionName() << \" \" <<\n                delete_object_outcome.GetError().GetMessage();\n-        LogError(_context, err.str().c_str());\n+        LogError(err.str().c_str());\n \n         return false;\n     }\n \n@@ -251,9 +251,9 @@\n     ss << \", failed: \" << req->GetFailedParts().size();\n     ss << \", pending: \" << req->GetPendingParts().size();\n     ss << \", queued: \" << req->GetQueuedParts().size();\n \n-    LogInfo(_context, ss.str().c_str());\n+    LogInfo(ss.str().c_str());\n }\n \n bool S3TransferManager::ConfigureAwsSdk(const std::string &s3_access_key, const std::string &s3_secret_key, const std::string &s3_bucket_name, const std::string &s3_region) {\n \n@@ -306,9 +306,9 @@\n \n     {\n         std::stringstream ss;\n         ss << \"[S3] Using tmp: \" << tempstr << '.';\n-        LogInfo(_context, ss.str());\n+        LogInfo(ss.str());\n     }\n \n     auto requestPtr = _tm->DownloadFile(_bucket_name,\n                                         path.c_str(),\n@@ -324,9 +324,9 @@\n             std::remove(tempstr.c_str());\n \n             std::stringstream ss;\n             ss << \"[S3] Failed to read file: \" << tempstr << \", \" << e.What();\n-            LogError(_context, ss.str());\n+            LogError(ss.str());\n \n             return false;\n         }\n     } else {\n@@ -354,15 +354,15 @@\n                                         path.c_str(),\n                                         [=](){\n         auto* out = Aws::New<Aws::IOStream>(ALLOCATION_TAG, buf.get());\n \n-        LogInfo(_context, \"Creating stream\");\n+        LogInfo(\"Creating stream\");\n         return out;\n     });\n \n     requestPtr->WaitUntilFinished();\n \n-    LogInfo (_context, \"Finished\");\n+    LogInfo (\"Finished\");\n \n     if (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED) {\n         LogDetails (requestPtr);\n \n@@ -382,20 +382,20 @@\n         ss << \"s1: \" << s1 << '\\n';\n         ss << \"s2: \" << s2 << '\\n';\n         ss << \"s3: \" << s3 << '\\n';\n         ss << \"s4: \" << s4 << '\\n';\n-        LogInfo(_context, ss.str().c_str());\n+        LogInfo(ss.str().c_str());\n \n         *content = buf->get();\n \n         if (*content == nullptr) {\n-            LogError(_context, \"Error allocating memory\");\n+            LogError(\"Error allocating memory\");\n             return false;\n         }\n \n-        //LogInfo(_context, \"Copying memory\");\n+        //LogInfo(\"Copying memory\");\n         //oss->rdbuf()->sgetn(static_cast<char*>(*content), *size);\n-        LogInfo(_context, \"DONE\");\n+        LogInfo(\"DONE\");\n     }\n \n     return (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED);\n }\n"
                }
            ],
            "date": 1628035160029,
            "name": "Commit-0",
            "content": "/**\n * S3 Storage Plugin - A plugin for Orthanc DICOM Server for storing\n * DICOM data in Amazon Simple Storage Service (AWS S3).\n *\n * Copyright (C) 2018 (Radpoint Sp. z o.o., Poland)\n * Marek Kwasecki, Bartłomiej Pyciński\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU Affero General Public License as\n * published by the Free Software Foundation, either version 3 of the\n * License, or (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU Affero General Public License for more details.\n *\n * You should have received a copy of the GNU Affero General Public License\n * along with this program.  If not, see <https://www.gnu.org/licenses/>.\n **/\n\n#include \"S3ops.hpp\"\n#include \"Utils.hpp\"\n//#include \"MemStreamBuf.hpp\"\n\n#include <aws/core/auth/AWSCredentialsProvider.h>\n#include <aws/s3/model/PutObjectRequest.h>\n#include <aws/s3/model/DeleteObjectRequest.h>\n#include <aws/s3/model/GetObjectRequest.h>\n#include <aws/s3/model/CreateBucketRequest.h>\n#include <aws/s3/model/GetBucketLocationRequest.h>\n\n#include <aws/core/utils/logging/DefaultLogSystem.h>\n#include <aws/core/utils/logging/ConsoleLogSystem.h>\n#include <aws/core/utils/FileSystemUtils.h>\n\n#include <boost/filesystem.hpp>\n#include <boost/interprocess/streams/bufferstream.hpp>\n\n#define ALLOCATION_TAG \"Orthanc_S3_Storage\"\n\nnamespace OrthancPlugins {\n\nS3Facade::S3Facade(S3Method m, OrthancPluginContext *context)\n{\n    switch(m) {\n    case S3Method::TRANSFER_MANAGER:\n        _s3 = std::unique_ptr<S3Impl>(new S3TransferManager(context));\n        break;\n    case S3Method::DIRECT:\n    default:\n        _s3 = std::unique_ptr<S3Impl>(new S3Direct(context));\n        break;\n    }\n}\n\nbool S3Impl::ConfigureAwsSdk(const std::string& s3_access_key,  const std::string& s3_secret_key,\n                               const std::string& s3_bucket_name, const std::string& s3_region) {\n\n    //Enable AWS logging\n    //Aws::Utils::Logging::InitializeAWSLogging(\n    //            Aws::MakeShared<Aws::Utils::Logging::DefaultLogSystem>( //TODO: change log system from file to std out\n    //                ALLOCATION_TAG, Aws::Utils::Logging::LogLevel::Info, \"aws_sdk_\"));\n    aws_api_options.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Info;\n    aws_api_options.loggingOptions.logger_create_fn = [] {\n        return Aws::MakeShared<Aws::Utils::Logging::ConsoleLogSystem>(ALLOCATION_TAG, Aws::Utils::Logging::LogLevel::Info);\n    };\n\n    Aws::InitAPI(aws_api_options);\n\n    Aws::Client::ClientConfiguration aws_client_config;\n    aws_client_config.region = s3_region.c_str();\n    aws_client_config.scheme = Aws::Http::Scheme::HTTPS;\n    aws_client_config.connectTimeoutMs = 30000;\n    aws_client_config.requestTimeoutMs = 600000;\n    aws_client_config.caPath = Aws::String(\"/etc/ssl/certs/\");\n\n    if (!s3_access_key.empty() && !s3_secret_key.empty()) {\n        LogInfo(_context, \"[S3] Using credentials from the config file\");\n        s3_client = Aws::MakeShared<Aws::S3::S3Client>(\n                    ALLOCATION_TAG,\n                    Aws::Auth::AWSCredentials(s3_access_key.c_str(), s3_secret_key.c_str()),\n                    aws_client_config,\n                    Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never, //signPayloads\n                    false //useVirtualAdressing\n                    );\n\n    } else {\n        LogInfo(_context, \"No credentials in the config file. Falling back to ~/.aws/credentials or env variables.\");\n        s3_client = Aws::MakeShared<Aws::S3::S3Client>(ALLOCATION_TAG,\n                                                       aws_client_config,\n                                                       Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never,\n                                                       false //useVirtualAdressing\n                                                       );\n    }\n\n    _bucket_name = s3_bucket_name.c_str();\n\n    std::stringstream ss;\n    ss <<  \"[S3] Checking bucket: \" << _bucket_name;\n    LogInfo(_context, ss.str().c_str());\n\n    //Create bucket if it doesn't exist\n    //and verify if it exists\n    Aws::S3::Model::CreateBucketRequest request;\n    request.SetBucket(_bucket_name);\n    Aws::S3::Model::CreateBucketConfiguration req_config;\n    // Only set location constraint if region is not default region (us-east-1)\n    auto regionConstraint = Aws::S3::Model::BucketLocationConstraintMapper::GetBucketLocationConstraintForName(s3_region.c_str());\n    if (regionConstraint != Aws::S3::Model::BucketLocationConstraint::us_east_1)\n    {\n        req_config.SetLocationConstraint(regionConstraint);\n    }\n    request.SetCreateBucketConfiguration(req_config);\n\n    auto outcome = s3_client->CreateBucket(request);\n\n    if (outcome.GetError().GetErrorType() == Aws::S3::S3Errors::BUCKET_ALREADY_OWNED_BY_YOU ||\n            outcome.GetError().GetErrorType() == Aws::S3::S3Errors::BUCKET_ALREADY_EXISTS ) {\n        std::stringstream ss;\n        ss << \"[S3] Bucket exists: \" << _bucket_name;\n        LogInfo(_context, ss.str().c_str());\n    } else if (outcome.IsSuccess()) {\n        std::stringstream ss;\n        ss << \"[S3] Bucket created: \" << _bucket_name;\n        LogInfo(_context, ss.str().c_str());\n    } else {\n        std::stringstream err;\n        err << \"[S3] Create Bucket error: \" <<\n               outcome.GetError().GetExceptionName() << \" \" <<\n               outcome.GetError().GetMessage();\n        LogError(_context, err.str().c_str());\n        return false;\n    }\n\n    return true;\n}\n\nbool S3Direct::UploadFileToS3(const std::string &path, const void *content, const int64_t &size) {\n    const Aws::String key_name = path.c_str();\n    const Aws::String file_name = path.c_str();\n\n    Aws::S3::Model::PutObjectRequest object_request;\n    object_request.WithBucket(_bucket_name).WithKey(key_name);\n\n    boost::interprocess::bufferstream buf(const_cast<char*>(static_cast<const char*>(content)), static_cast<size_t>(size));\n    auto body = Aws::MakeShared<Aws::IOStream>(ALLOCATION_TAG, buf.rdbuf());\n\n    //std::stringbuf buf(const_cast<char*>(static_cast<const char*>(content)), size);\n    //std::shared_ptr<Aws::IOStream> body = Aws::MakeShared<Aws::IOStream>(ALLOCATION_TAG, &buf);\n\n    auto input_data = Aws::MakeShared<Aws::FStream>(\"PutObjectInputStream\",\n                                                    file_name.c_str(), std::ios_base::in | std::ios_base::binary);\n\n    object_request.SetBody(body);\n\n    auto put_object_outcome = s3_client->PutObject(object_request);\n\n    if (!put_object_outcome.IsSuccess()) {\n        std::stringstream err;\n        err << \"[S3] PUT error: \" <<\n               put_object_outcome.GetError().GetExceptionName() << \" \" <<\n               put_object_outcome.GetError().GetMessage();\n        LogError(_context, err.str().c_str());\n\n        return false;\n    }\n\n    return true;\n\n}\n\nbool S3Direct::DownloadFileFromS3(const std::string &path, void **content, int64_t *size) {\n    const Aws::String key_name = path.c_str();\n\n    Aws::S3::Model::GetObjectRequest object_request;\n    object_request.WithBucket(_bucket_name).WithKey(key_name);\n\n    auto get_object_outcome = s3_client->GetObject(object_request);\n\n    if (get_object_outcome.IsSuccess()) {\n        *size = get_object_outcome.GetResult().GetContentLength();\n\n        Aws::OStringStream buf;\n        //malloc because it's freed by ::free()\n        *content = malloc(static_cast<size_t>(*size));\n\n        if (*content!=nullptr) {\n            buf.rdbuf()->pubsetbuf(static_cast<char*>(*content), *size);\n            buf << get_object_outcome.GetResult().GetBody().rdbuf();\n\n            //memcpy(*content, buf.str().c_str(), *size);\n        } else {\n            return false;\n        }\n    } else {\n        std::stringstream err;\n        err << \"[S3] GET error: \" <<\n               get_object_outcome.GetError().GetExceptionName() << \" \" <<\n               get_object_outcome.GetError().GetMessage();\n        LogError(_context, err.str().c_str());\n\n        return false;\n    }\n\n    return true;\n}\n\nbool S3Direct::DeleteFileFromS3(const std::string &path) {\n    const Aws::String key_name = path.c_str();\n\n    Aws::S3::Model::DeleteObjectRequest object_request;\n    object_request.WithBucket(_bucket_name).WithKey(key_name);\n\n    auto delete_object_outcome = s3_client->DeleteObject(object_request);\n\n    if (!delete_object_outcome.IsSuccess()) {\n        std::stringstream err;\n        err << \"[S3] DELETE error: \" <<\n               delete_object_outcome.GetError().GetExceptionName() << \" \" <<\n               delete_object_outcome.GetError().GetMessage();\n        LogError(_context, err.str().c_str());\n\n        return false;\n    }\n\n    return true;\n}\n\n/*\n * Transfer Manager Implementation\n */\n\nvoid S3TransferManager::LogDetails(const std::shared_ptr<const Aws::Transfer::TransferHandle>& req) {\n    std::stringstream ss;\n\n    ss << \"Status: \";\n    switch (req->GetStatus()) {\n    case Aws::Transfer::TransferStatus::ABORTED: ss << \"Aborted\"; break;\n    case Aws::Transfer::TransferStatus::CANCELED: ss << \"Canceled\"; break;\n    case Aws::Transfer::TransferStatus::COMPLETED: ss << \"Completed\"; break;\n    case Aws::Transfer::TransferStatus::EXACT_OBJECT_ALREADY_EXISTS: ss << \"Already Exists\"; break;\n    case Aws::Transfer::TransferStatus::FAILED: ss << \"Failed\"; break;\n    case Aws::Transfer::TransferStatus::IN_PROGRESS: ss << \"In Progress\"; break;\n    case Aws::Transfer::TransferStatus::NOT_STARTED: ss << \"Not Started\"; break;\n    default:\n        break;\n    }\n    ss << \". \";\n    ss << \"completed: \" << req->GetCompletedParts().size(); // Should be tiny\n    ss << \", failed: \" << req->GetFailedParts().size();\n    ss << \", pending: \" << req->GetPendingParts().size();\n    ss << \", queued: \" << req->GetQueuedParts().size();\n\n    LogInfo(_context, ss.str().c_str());\n}\n\nbool S3TransferManager::ConfigureAwsSdk(const std::string &s3_access_key, const std::string &s3_secret_key, const std::string &s3_bucket_name, const std::string &s3_region) {\n\n    S3Impl::ConfigureAwsSdk(s3_access_key, s3_secret_key, s3_bucket_name, s3_region);\n\n    _executor = Aws::MakeShared<Aws::Utils::Threading::PooledThreadExecutor>(ALLOCATION_TAG, 4);\n    Aws::Transfer::TransferManagerConfiguration transferConfig(_executor.get());\n    transferConfig.s3Client = s3_client;\n\n    transferConfig.errorCallback = [&](const Aws::Transfer::TransferManager*, const std::shared_ptr<const Aws::Transfer::TransferHandle>& req, const Aws::Client::AWSError<Aws::S3::S3Errors>& e) {\n        std::stringstream ss;\n        ss << \"[S3] Error: \" << e.GetMessage() << '.';\n\n        LogDetails(req);\n    };\n\n    _tm = Aws::Transfer::TransferManager::Create(transferConfig);\n\n    return true;\n}\n\nbool S3TransferManager::UploadFileToS3(const std::string &path, const void *content, const int64_t &size) {\n    boost::interprocess::bufferstream buf(const_cast<char*>(static_cast<const char*>(content)), static_cast<size_t>(size));\n    auto body = Aws::MakeShared<Aws::IOStream>(ALLOCATION_TAG, buf.rdbuf());\n\n    auto requestPtr = _tm->UploadFile(body,\n                                      _bucket_name,\n                                      path.c_str(),\n                                      \"text/plain\",\n                                      Aws::Map<Aws::String, Aws::String>());\n\n    requestPtr->WaitUntilFinished();\n\n    size_t retries = 0;\n    while (requestPtr->GetStatus() != Aws::Transfer::TransferStatus::COMPLETED && retries++ < 5)\n    {\n        _tm->RetryUpload(body, requestPtr);\n        requestPtr->WaitUntilFinished();\n    }\n\n    LogDetails(requestPtr);\n\n    return (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED);\n}\n\nbool S3TransferManager::DownloadFileFromS3(const std::string &path, void **content, int64_t *size) {\n\n    boost::filesystem::path temp = \"/tmp\" / boost::filesystem::unique_path();\n    const std::string tempstr    = temp.native();  // optional\n\n    {\n        std::stringstream ss;\n        ss << \"[S3] Using tmp: \" << tempstr << '.';\n        LogInfo(_context, ss.str());\n    }\n\n    auto requestPtr = _tm->DownloadFile(_bucket_name,\n                                        path.c_str(),\n                                        tempstr.c_str());\n\n    requestPtr->WaitUntilFinished();\n\n    if (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED) {\n        //read file to memory\n        try {\n            Utils::readFile(content, size, tempstr);\n        } catch (Orthanc::OrthancException &e) {\n            std::remove(tempstr.c_str());\n\n            std::stringstream ss;\n            ss << \"[S3] Failed to read file: \" << tempstr << \", \" << e.What();\n            LogError(_context, ss.str());\n\n            return false;\n        }\n    } else {\n        std::stringstream ss;\n        auto err = requestPtr->GetLastError();\n        ss << \"[S3] Failed to get file: \" << path << \" because of: \" << err.GetMessage() <<'.';\n        LogError(_context, ss.str());\n    }\n\n    std::remove(tempstr.c_str());\n\n    return (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED);\n\n\n}\n\n/*\n * TODO: fix the memory transfer\nbool S3TransferManager::DownloadFileFromS3(const std::string &path, void **content, int64_t *size) {\n\n    auto buf = Aws::MakeShared<Stream::MemStreamBuf>(ALLOCATION_TAG, nullptr, 0, false);\n    auto oss = std::make_shared<std::istream>(buf.get());\n\n    auto requestPtr = _tm->DownloadFile(_bucket_name,\n                                        path.c_str(),\n                                        [=](){\n        auto* out = Aws::New<Aws::IOStream>(ALLOCATION_TAG, buf.get());\n\n        LogInfo(_context, \"Creating stream\");\n        return out;\n    });\n\n    requestPtr->WaitUntilFinished();\n\n    LogInfo (_context, \"Finished\");\n\n    if (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED) {\n        LogDetails (requestPtr);\n\n        //assert(buf->size() == requestPtr->GetBytesTotalSize());\n        size_t s1 = buf->size();\n        size_t s2 = buf->allocsize();\n        size_t s3 = requestPtr->GetBytesTransferred();\n        size_t s4 = requestPtr->GetBytesTotalSize();\n        *size = requestPtr->GetBytesTotalSize();\n\n        //oss->seekp(0, std::ios::end);\n        // *size = oss->tellp();\n        //oss->seekp(0);//, std::ios::beg);\n\n        std::stringstream ss;\n        ss << \"Size: \" << *size << '\\n';\n        ss << \"s1: \" << s1 << '\\n';\n        ss << \"s2: \" << s2 << '\\n';\n        ss << \"s3: \" << s3 << '\\n';\n        ss << \"s4: \" << s4 << '\\n';\n        LogInfo(_context, ss.str().c_str());\n\n        *content = buf->get();\n\n        if (*content == nullptr) {\n            LogError(_context, \"Error allocating memory\");\n            return false;\n        }\n\n        //LogInfo(_context, \"Copying memory\");\n        //oss->rdbuf()->sgetn(static_cast<char*>(*content), *size);\n        LogInfo(_context, \"DONE\");\n    }\n\n    return (requestPtr->GetStatus() == Aws::Transfer::TransferStatus::COMPLETED);\n}\n*/\n\nbool S3TransferManager::DeleteFileFromS3(const std::string &path) {\n    const Aws::String key_name = path.c_str();\n\n    Aws::S3::Model::DeleteObjectRequest object_request;\n    object_request.WithBucket(_bucket_name).WithKey(key_name);\n\n    auto delete_object_outcome = s3_client->DeleteObject(object_request);\n\n    if (!delete_object_outcome.IsSuccess()) {\n        std::stringstream err;\n        err << \"[S3] DELETE error: \" <<\n               delete_object_outcome.GetError().GetExceptionName() << \" \" <<\n               delete_object_outcome.GetError().GetMessage();\n        LogError(err.str().c_str());\n\n        return false;\n    }\n\n    return true;\n}\n\n\n}\n"
        }
    ]
}